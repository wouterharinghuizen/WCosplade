{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -U transformers torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\hoiho\\.conda\\envs\\ir2\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForMaskedLM, AutoTokenizer\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use a pretrained model from [huggingface](https://huggingface.co/naver/splade-cocondenser-selfdistil)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_id = 'naver/splade-cocondenser-ensembledistil'\n",
    "model_id = 'naver/splade-cocondenser-selfdistil'\n",
    "\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModelForMaskedLM.from_pretrained(model_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# test text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = 'test text'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " tokenized text: ['[CLS]', 'test', 'text', '[SEP]']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MaskedLMOutput(loss=None, logits=tensor([[[ -6.3943,  -7.9640,  -7.5507,  ...,  -7.7696,  -7.7066,  -6.0523],\n",
       "         [-36.7605, -25.9343, -22.5490,  ..., -25.1820, -26.5855, -27.3972],\n",
       "         [-26.7355, -20.2880, -22.2167,  ..., -19.8303, -19.0085, -24.0596],\n",
       "         [-19.8575, -15.9112, -15.6052,  ..., -15.7236, -15.2050, -16.4795]]],\n",
       "       grad_fn=<ViewBackward0>), hidden_states=None, attentions=None)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = tokenizer(text, return_tensors='pt')\n",
    "print(f' tokenized text: {tokenizer.convert_ids_to_tokens(tokens[\"input_ids\"][0])}')\n",
    "output = model(**tokens)\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 4, 30522])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.logits.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have a probability distribution over all token, but we want it over the entire text, the splade paper does this by:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([30522])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "vec = torch.max(\n",
    "    torch.log(\n",
    "        1 + torch.relu(output.logits)\n",
    "    ) * tokens.attention_mask.unsqueeze(-1),\n",
    "dim=1)[0].squeeze()\n",
    "\n",
    "vec.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 0., 0.,  ..., 0., 0., 0.], grad_fn=<SqueezeBackward0>)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "amount of non-zero values: 58\n",
      "the non-zero values:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{2508: 0.1254245936870575,\n",
       " 2653: 0.042251672595739365,\n",
       " 2731: 0.12503007054328918,\n",
       " 2739: 0.01955387368798256,\n",
       " 2773: 0.09141194820404053,\n",
       " 3076: 0.16296613216400146,\n",
       " 3160: 0.309316486120224,\n",
       " 3189: 0.15591995418071747,\n",
       " 3231: 2.970235586166382,\n",
       " 3259: 0.0020148707553744316,\n",
       " 3350: 0.1404428482055664,\n",
       " 3433: 0.20468543469905853,\n",
       " 3485: 0.5482600927352905,\n",
       " 3642: 0.09496811777353287,\n",
       " 3661: 0.5951390266418457,\n",
       " 3752: 0.48842114210128784,\n",
       " 3793: 2.7863576412200928,\n",
       " 3836: 0.1300884634256363,\n",
       " 3945: 0.0756244882941246,\n",
       " 4106: 0.1932976394891739,\n",
       " 4289: 0.03775309771299362,\n",
       " 4345: 0.015433406457304955,\n",
       " 4357: 0.5832338929176331,\n",
       " 4431: 0.08798141032457352,\n",
       " 4471: 0.5469196438789368,\n",
       " 4613: 0.22698090970516205,\n",
       " 4742: 0.09591808915138245,\n",
       " 4807: 0.38130977749824524,\n",
       " 4918: 0.21712850034236908,\n",
       " 5074: 0.04295560345053673,\n",
       " 5604: 2.032471179962158,\n",
       " 5616: 0.012767007574439049,\n",
       " 5896: 0.20169223845005035,\n",
       " 6254: 0.5167056918144226,\n",
       " 6498: 0.13640891015529633,\n",
       " 6868: 0.02545907348394394,\n",
       " 6981: 2.3848140239715576,\n",
       " 7099: 0.3119748532772064,\n",
       " 7655: 0.1497763991355896,\n",
       " 7667: 0.452457070350647,\n",
       " 7908: 0.03894691541790962,\n",
       " 7961: 0.1286001205444336,\n",
       " 8035: 0.36192265152931213,\n",
       " 8262: 0.015942487865686417,\n",
       " 8756: 0.18327918648719788,\n",
       " 8785: 0.1267804056406021,\n",
       " 8861: 0.28377777338027954,\n",
       " 9986: 0.016490474343299866,\n",
       " 10013: 0.054589323699474335,\n",
       " 11291: 0.024740271270275116,\n",
       " 11360: 1.1226441860198975,\n",
       " 12874: 0.06076553091406822,\n",
       " 13594: 0.16223156452178955,\n",
       " 14324: 0.3352324366569519,\n",
       " 14686: 0.6376148462295532,\n",
       " 14924: 0.03489089757204056,\n",
       " 19461: 0.12444256991147995,\n",
       " 26664: 0.15614432096481323}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# extract non-zero positions\n",
    "cols = vec.nonzero().squeeze().cpu().tolist()\n",
    "print(f\"amount of non-zero values: {len(cols)}\")\n",
    "\n",
    "# extract the non-zero values\n",
    "weights = vec[cols].cpu().tolist()\n",
    "# use to create a dictionary of token ID to weight\n",
    "sparse_dict = dict(zip(cols, weights))\n",
    "\n",
    "print(\"the non-zero values:\")\n",
    "sparse_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These tokens do not tell us much lets map them back to tokens:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract the ID position to text token mappings\n",
    "idx2token = {\n",
    "    idx: token for token, idx in tokenizer.get_vocab().items()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'test': 2.97,\n",
       " 'text': 2.79,\n",
       " 'texts': 2.38,\n",
       " 'testing': 2.03,\n",
       " 'exam': 1.12,\n",
       " 'quote': 0.64,\n",
       " 'letter': 0.6,\n",
       " 'interview': 0.58,\n",
       " 'journal': 0.55,\n",
       " 'message': 0.55,\n",
       " 'document': 0.52,\n",
       " 'reading': 0.49,\n",
       " 'assessment': 0.45,\n",
       " 'communication': 0.38,\n",
       " 'grammar': 0.36,\n",
       " 'bert': 0.34,\n",
       " 'question': 0.31,\n",
       " 'sample': 0.31,\n",
       " 'malcolm': 0.28,\n",
       " 'speech': 0.23,\n",
       " 'charlie': 0.22,\n",
       " 'response': 0.2,\n",
       " 'script': 0.2,\n",
       " 'analysis': 0.19,\n",
       " 'archive': 0.18,\n",
       " 'student': 0.16,\n",
       " 'report': 0.16,\n",
       " 'scan': 0.16,\n",
       " 'phonetic': 0.16,\n",
       " 'phrase': 0.15,\n",
       " 'evidence': 0.14,\n",
       " 'josh': 0.14,\n",
       " 'james': 0.13,\n",
       " 'training': 0.13,\n",
       " 'teacher': 0.13,\n",
       " 'logic': 0.13,\n",
       " 'math': 0.13,\n",
       " 'quiz': 0.12,\n",
       " 'signal': 0.1,\n",
       " 'word': 0.09,\n",
       " 'code': 0.09,\n",
       " 'reference': 0.09,\n",
       " 'search': 0.08,\n",
       " 'pearson': 0.06,\n",
       " 'telegraph': 0.05,\n",
       " 'language': 0.04,\n",
       " 'format': 0.04,\n",
       " 'roger': 0.04,\n",
       " 'watson': 0.04,\n",
       " 'collins': 0.03,\n",
       " 'tutor': 0.03,\n",
       " 'news': 0.02,\n",
       " 'oxford': 0.02,\n",
       " 'roland': 0.02,\n",
       " 'doc': 0.02,\n",
       " 'toby': 0.02,\n",
       " 'emma': 0.01,\n",
       " 'paper': 0.0}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# map token IDs to human-readable tokens\n",
    "sparse_dict_tokens = {\n",
    "    idx2token[idx]: round(weight, 2) for idx, weight in zip(cols, weights)\n",
    "}\n",
    "# sort so we can see most relevant tokens first\n",
    "sparse_dict_tokens = {\n",
    "    k: v for k, v in sorted(\n",
    "        sparse_dict_tokens.items(),\n",
    "        key=lambda item: item[1],\n",
    "        reverse=True\n",
    "    )\n",
    "}\n",
    "sparse_dict_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparing vectors\n",
    "\n",
    "We will now compare 3 pieces of text to eachother to see how that works:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = [\n",
    "   \"information retrieval is hard to understand, but lovely when you understand it.\",\n",
    "   \"I love going to the University of Amsterdam\",\n",
    "   \"I don't want to go to school mum... we need to do information retrieval\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 30522)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = tokenizer(\n",
    "    texts, return_tensors='pt',\n",
    "    padding=True, truncation=True\n",
    ")\n",
    "output = model(**tokens)\n",
    "# aggregate the token-level vecs and transform to sparse\n",
    "vecs = torch.max(\n",
    "    torch.log(1 + torch.relu(output.logits)) * tokens.attention_mask.unsqueeze(-1), dim=1\n",
    ")[0].squeeze().detach().cpu().numpy()\n",
    "vecs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "sim = np.zeros((vecs.shape[0], vecs.shape[0]))\n",
    "\n",
    "for i, vec in enumerate(vecs):\n",
    "    sim[i,:] = np.dot(vec, vecs.T) / (\n",
    "        np.linalg.norm(vec) * np.linalg.norm(vecs, axis=1)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.        , 0.01146347, 0.39074919],\n",
       "       [0.01146347, 1.00000012, 0.16770877],\n",
       "       [0.39074919, 0.16770874, 1.        ]])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
