{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -U transformers torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForMaskedLM, AutoTokenizer\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = 'naver/splade-cocondenser-ensembledistil'\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModelForMaskedLM.from_pretrained(model_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# test text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = 'test text'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " tokenized text: ['[CLS]', 'test', 'text', '[SEP]']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MaskedLMOutput(loss=None, logits=tensor([[[ -6.1641,  -8.0746,  -7.6957,  ...,  -7.7311,  -7.7196,  -5.3543],\n",
       "         [-35.6677, -25.3951, -22.7048,  ..., -24.4138, -25.9929, -26.7460],\n",
       "         [-22.1844, -18.0183, -19.8789,  ..., -17.4275, -16.6500, -19.5127],\n",
       "         [-18.3353, -15.3200, -14.5211,  ..., -14.9485, -14.1121, -15.8339]]],\n",
       "       grad_fn=<ViewBackward0>), hidden_states=None, attentions=None)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = tokenizer(text, return_tensors='pt')\n",
    "print(f' tokenized text: {tokenizer.convert_ids_to_tokens(tokens[\"input_ids\"][0])}')\n",
    "output = model(**tokens)\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 4, 30522])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.logits.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have a probability distribution over all token, but we want it over the entire text, the splade paper does this by:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([30522])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "vec = torch.max(\n",
    "    torch.log(\n",
    "        1 + torch.relu(output.logits)\n",
    "    ) * tokens.attention_mask.unsqueeze(-1),\n",
    "dim=1)[0].squeeze()\n",
    "\n",
    "vec.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 0., 0.,  ..., 0., 0., 0.], grad_fn=<SqueezeBackward0>)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "amount of non-zero values: 33\n",
      "the non-zero values:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{2726: 0.20849496126174927,\n",
       " 2773: 0.09160938858985901,\n",
       " 3076: 0.20714879035949707,\n",
       " 3189: 0.3412291705608368,\n",
       " 3231: 3.0339627265930176,\n",
       " 3648: 0.0031361228320747614,\n",
       " 3661: 0.3299848735332489,\n",
       " 3752: 0.2602483034133911,\n",
       " 3793: 2.7784647941589355,\n",
       " 3836: 0.06602185219526291,\n",
       " 4180: 0.4668448269367218,\n",
       " 4405: 0.01899118907749653,\n",
       " 4471: 0.3733828067779541,\n",
       " 4918: 0.025326814502477646,\n",
       " 5074: 0.1128750592470169,\n",
       " 5604: 2.3174471855163574,\n",
       " 5852: 0.5542181730270386,\n",
       " 6140: 0.0016638495726510882,\n",
       " 6254: 0.31116071343421936,\n",
       " 6845: 0.12719713151454926,\n",
       " 6981: 1.8357378244400024,\n",
       " 7099: 0.3066442012786865,\n",
       " 7551: 0.09702988713979721,\n",
       " 7667: 0.4796116352081299,\n",
       " 8744: 0.2446175217628479,\n",
       " 8785: 0.2586182653903961,\n",
       " 10618: 0.13413843512535095,\n",
       " 11360: 1.4634112119674683,\n",
       " 12874: 0.5858922004699707,\n",
       " 14686: 0.283345490694046,\n",
       " 19461: 0.05838468298316002,\n",
       " 22498: 0.3845391273498535,\n",
       " 28770: 0.2040555477142334}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# extract non-zero positions\n",
    "cols = vec.nonzero().squeeze().cpu().tolist()\n",
    "print(f\"amount of non-zero values: {len(cols)}\")\n",
    "\n",
    "# extract the non-zero values\n",
    "weights = vec[cols].cpu().tolist()\n",
    "# use to create a dictionary of token ID to weight\n",
    "sparse_dict = dict(zip(cols, weights))\n",
    "\n",
    "print(\"the non-zero values:\")\n",
    "sparse_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These tokens do not tell us much lets map them back to tokens:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract the ID position to text token mappings\n",
    "idx2token = {\n",
    "    idx: token for token, idx in tokenizer.get_vocab().items()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'test': 3.03,\n",
       " 'text': 2.78,\n",
       " 'testing': 2.32,\n",
       " 'texts': 1.84,\n",
       " 'exam': 1.46,\n",
       " 'pearson': 0.59,\n",
       " 'tests': 0.55,\n",
       " 'assessment': 0.48,\n",
       " 'content': 0.47,\n",
       " 'abbreviation': 0.38,\n",
       " 'message': 0.37,\n",
       " 'report': 0.34,\n",
       " 'letter': 0.33,\n",
       " 'document': 0.31,\n",
       " 'sample': 0.31,\n",
       " 'quote': 0.28,\n",
       " 'reading': 0.26,\n",
       " 'math': 0.26,\n",
       " 'blank': 0.24,\n",
       " 'thomas': 0.21,\n",
       " 'student': 0.21,\n",
       " 'proctor': 0.2,\n",
       " 'lab': 0.13,\n",
       " 'certification': 0.13,\n",
       " 'roger': 0.11,\n",
       " 'experiment': 0.1,\n",
       " 'word': 0.09,\n",
       " 'teacher': 0.07,\n",
       " 'quiz': 0.06,\n",
       " 'charlie': 0.03,\n",
       " 'pilot': 0.02,\n",
       " 'judge': 0.0,\n",
       " 'print': 0.0}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# map token IDs to human-readable tokens\n",
    "sparse_dict_tokens = {\n",
    "    idx2token[idx]: round(weight, 2) for idx, weight in zip(cols, weights)\n",
    "}\n",
    "# sort so we can see most relevant tokens first\n",
    "sparse_dict_tokens = {\n",
    "    k: v for k, v in sorted(\n",
    "        sparse_dict_tokens.items(),\n",
    "        key=lambda item: item[1],\n",
    "        reverse=True\n",
    "    )\n",
    "}\n",
    "sparse_dict_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparing vectors\n",
    "\n",
    "We will now compare 3 pieces of text to eachother to see how that works:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = [\n",
    "   \"information retrieval is hard to understand, but lovely when you understand it.\",\n",
    "   \"I love going to the University of Amsterdam\",\n",
    "   \"I don't want to go to school mum... we need to do information retrieval\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 30522)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = tokenizer(\n",
    "    texts, return_tensors='pt',\n",
    "    padding=True, truncation=True\n",
    ")\n",
    "output = model(**tokens)\n",
    "# aggregate the token-level vecs and transform to sparse\n",
    "vecs = torch.max(\n",
    "    torch.log(1 + torch.relu(output.logits)) * tokens.attention_mask.unsqueeze(-1), dim=1\n",
    ")[0].squeeze().detach().cpu().numpy()\n",
    "vecs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "sim = np.zeros((vecs.shape[0], vecs.shape[0]))\n",
    "\n",
    "for i, vec in enumerate(vecs):\n",
    "    sim[i,:] = np.dot(vec, vecs.T) / (\n",
    "        np.linalg.norm(vec) * np.linalg.norm(vecs, axis=1)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.00000012, 0.01163663, 0.33802783],\n",
       "       [0.01163663, 1.        , 0.17227599],\n",
       "       [0.33802783, 0.17227599, 1.        ]])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
