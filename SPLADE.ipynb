{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## In this jupyter notebook we will calculate the SPLADE representation of the trec-cast dataset (just the queries)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we will download the trec-cast v1 2020 dataset from [ir-datasets.com](https://ir-datasets.com/trec-cast.html#trec-cast/v1/2020)\n",
    "In this JN we only look at the queries, since running splade on all documents takes way to long to run locally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "amount of topics\t: \t len(topics)\n",
      "amount of queries\t: \t 216\n"
     ]
    }
   ],
   "source": [
    "import ir_datasets\n",
    "dataset = ir_datasets.load(\"trec-cast/v1/2020\")\n",
    "topics = set()\n",
    "for query in dataset.queries_iter():\n",
    "    topics.add(query.topic_number)\n",
    "\n",
    "print(f\"amount of topics\\t: \\t len(topics)\")\n",
    "print(f\"amount of queries\\t: \\t {len(dataset.queries)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets initialize the model (from huggingface, see SPLADE test for more info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForMaskedLM, AutoTokenizer\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets run the code on the queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import csv\n",
    "\n",
    "def process_queries_and_write_tsv_file(model_id, dataset, N, utterance_type, output_file_path):\n",
    "    from transformers import AutoTokenizer, AutoModelForMaskedLM\n",
    "\n",
    "    # Load tokenizer and model\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "    model = AutoModelForMaskedLM.from_pretrained(model_id)\n",
    "\n",
    "    idx2token = {\n",
    "        idx: token for token, idx in tokenizer.get_vocab().items()\n",
    "    }\n",
    "\n",
    "    results = []\n",
    "\n",
    "    for query in dataset.queries_iter():\n",
    "        tokens = tokenizer(getattr(query, utterance_type), return_tensors='pt')\n",
    "        output = model(**tokens)\n",
    "        query_id = query.query_id\n",
    "\n",
    "        vec = torch.max(\n",
    "            torch.log(\n",
    "                1 + torch.relu(output.logits)\n",
    "            ) * tokens.attention_mask.unsqueeze(-1),\n",
    "            dim=1)[0].squeeze()\n",
    "\n",
    "        cols = vec.nonzero().squeeze().cpu().tolist()\n",
    "\n",
    "        # Extract the non-zero values\n",
    "        weights = vec[cols].cpu().tolist()\n",
    "        # Create a dictionary of token ID to weight\n",
    "        sparse_dict = dict(zip(cols, weights))\n",
    "\n",
    "        # Sort the dictionary by weight in descending order\n",
    "        sparse_dict = dict(sorted(sparse_dict.items(), key=lambda item: item[1], reverse=True))\n",
    "\n",
    "        # Convert the token IDs to words\n",
    "        sparse_dict_tokens = [\n",
    "            idx2token[idx] for idx in cols for _ in range(int(round(sparse_dict[idx] * N, 0)))\n",
    "        ]\n",
    "\n",
    "        # Generate the output string in the desired format\n",
    "        output_str = f\"{query_id}\\t{' '.join(sparse_dict_tokens)}\"\n",
    "        results.append(output_str)\n",
    "\n",
    "    # Write the results to a TSV file\n",
    "    with open(output_file_path, \"w\", newline=\"\", encoding=\"utf-8\") as tsvfile:\n",
    "        writer = csv.writer(tsvfile, delimiter='\\t')\n",
    "        writer.writerows([line.split('\\t') for line in results])\n",
    "\n",
    "# Usage example for N=1 and N=100, and three utterance types\n",
    "model_id = 'naver/splade-cocondenser-selfdistil'\n",
    "output_dir = 'SPLADE_embeddings'\n",
    "\n",
    "# for N in [1, 100]:\n",
    "#     for utterance_type in [\"raw_utterance\", \"automatic_rewritten_utterance\", \"manual_rewritten_utterance\"]:\n",
    "#         output_file_path = f\"{output_dir}/output_N{N}_{utterance_type}.tsv\"\n",
    "#         process_queries_and_write_tsv_file(model_id, dataset, N, utterance_type, output_file_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ir2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
